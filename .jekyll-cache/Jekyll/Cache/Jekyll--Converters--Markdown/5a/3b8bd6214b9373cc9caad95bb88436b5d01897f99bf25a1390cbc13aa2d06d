I"˜C<p>Today we will build a classification model, use it to make a prediction and evaluate how well our classifier copes with a given problem. So letâ€™s not waste time and get to work!</p>

<!--more-->

<h2 id="dataset-preparation">Dataset preparation</h2>

<h3 id="loading-dataset">Loading dataset</h3>

<p>In the previous examples, we learned how to generate a synthetic data set, save it to a CSV file and load such files. So letâ€™s start todayâ€™s task by loading the previously prepared dataset, which is located in the <code class="language-plaintext highlighter-rouge">dataset.csv</code> file (conveniently available for download <a href="https://github.com/metsi/metsi.github.io/blob/master/examples/dataset.csv">here</a>).</p>

<p>The data set was generated using a (well-known I hope) code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">flip_y</span><span class="o">=</span><span class="p">.</span><span class="mi">05</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1410</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Loading the problem and its division into <em>data set</em> and <em>set of labels</em>, as a reminder, is performed as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">"dataset.csv"</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">","</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</code></pre></div></div>

<p>We have our â€˜Xâ€™ and â€˜yâ€™ ready, but if we want our research to bear the hallmarks of reliability, then we cannot stop there.</p>

<h3 id="division-of-data-into-a-training-and-a-test-set">Division of data into a training and a test set</h3>

<p>Before doing any research, we must remember to divide our dataset into <em>training set</em> and <em>test set</em>. In the classification task, the <em>training set</em> contains the instances of the problem known to us, which have been previously labeled and will be used to train the model we have chosen, so that it can then generalize the acquired knowledge in relation to instances that it has not seen before (i.e. <em>test set</em>). Training and testing a model on the same data set is absolutely unacceptable and the results obtained in this way do not in any way reflect the actual predictive ability of the classifier.</p>

<p>In todayâ€™s example, we will use the <code class="language-plaintext highlighter-rouge">train_test_split()</code> function, with which we will perform a single partition of our data set. This approach is clearly intellectually unobtrusive and insulting to the dignity of a scientist, but we will present it here (<strong>and only here</strong>) as the simplest possible example of data division into the classification task. Next time, we will learn to use <em>cross-validation</em>, which will allow us to conduct our research reliably, and this is what we will use in all future machine learning encounters.</p>

<p>But back to the present, letâ€™s prepare our <em>test set</em> and <em>training set</em> using <code class="language-plaintext highlighter-rouge">train_test_split()</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="p">.</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</code></pre></div></div>

<p>In this case, we only need to pass two parameters as arguments to the function, i.e. well-known and liked <code class="language-plaintext highlighter-rouge">random_state</code> and <code class="language-plaintext highlighter-rouge">test_size</code>, which tells us what percentage of all data will be used to test our model. Here we decide to use 70% of the data for training and 30% for testing the classifier.</p>

<h2 id="classification">Classification</h2>

<h3 id="initialization-and-construction-of-classification-model">Initialization and construction of classification model</h3>

<p>Now that we have our data ready, we can finally move on to initializing and training the classification model. We will use a simple probabilistic classifier, which is the <em>Naive Bayes classifier</em> in the version based on the normal distribution (Gaussian distribution). It is called <em>naive</em> because it assumes that the problem features are independent of each other and is implemented by the <code class="language-plaintext highlighter-rouge">GaussianNB</code> class.</p>

<p>In order to fit our model to the training data, we call the <code class="language-plaintext highlighter-rouge">fit()</code> method, which takes the <em>data set</em> and <em>label set</em> of our training set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>Our model has already been learned from the labeled data, so we can finally proceed with the prediction.</p>

<h3 id="determination-of-the-support-and-prediction-matrix">Determination of the support and prediction matrix</h3>

<p>The classifiers implemented in the <code class="language-plaintext highlighter-rouge">scikit-learn</code> library implement the <code class="language-plaintext highlighter-rouge">predict()</code> method, which allows us to directly obtain labels assigned to unknown instances by the classifier. However, to prevent it from being so easy, we will not use it.</p>

<p>Due to the fact that the model used by us is probabilistic, we can ask him to return the <em>support matrix</em>. The number of rows in this matrix is equal to the number of problem instances in the <em>test set</em> (i.e. we have 30), and the number of columns is equal to the number of problem classes (in our case, the problem is binary, so we have two columns). Each row of the matrix corresponds to <em>the support vector</em> returned by the model for a given classified problem instance. The values in this vector tell us with what probability the classified instance belongs to a given class. <em>The support matrix</em> can be obtained using the <code class="language-plaintext highlighter-rouge">predict_proba()</code> method and giving our test data set as its argument:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Class probabilities:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">class_probabilities</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; Class probabilities:
&gt;&gt; [[2.57883436e-06 9.99997421e 01]
&gt;&gt; [1.99658559e-06 9.99998003e-01]
&gt;&gt; [9.90973563e-01 9.02643748e-03]
&gt;&gt; [5.47593735e-05 9.99945241e-01]
&gt;&gt; [9.98382405e-01 1.61759531e-03]
...
</code></pre></div></div>

<p>Having knowledge of the probabilities of the <em>test set</em> instance belonging to classes (posterior probability), we can easily calculate the prediction. We will do this assuming that each of the test instances belongs to the problem class for which the probability of belonging is the highest.</p>

<p>We can do this very efficiently with the <code class="language-plaintext highlighter-rouge">argmax()</code> function, which will return to us the index of the column with the largest value for each row. For this purpose, we give as arguments the previously obtained <em>support matrix</em> and the axis on which we want to move:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predict</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">class_probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Predicted labels:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; Predicted labels:
&gt;&gt; [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
</code></pre></div></div>

<p>For purely illustrative purposes, we will now list the true labels of our <em>test set</em> (<code class="language-plaintext highlighter-rouge">y_test</code>) and our prediction (<code class="language-plaintext highlighter-rouge">predict</code>) next to each other:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"True labels:     "</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Predicted labels:"</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; True labels:      [1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
&gt;&gt; Predicted labels: [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
</code></pre></div></div>

<p>As we are dealing here with only 30 instances of the problem, at first glance we can say that the two vectors are slightly different from each other. This shows that our prediction is not perfect (and in practice it will never be perfect), but of course it would be inconvenient to judge how our model works in this way. We need a specific value that will tell us how good our classifier is in this case.</p>

<h3 id="assessment-of-the-classification-quality-of-our-model">Assessment of the classification quality of our model</h3>

<p>We managed to get to the last issue of today, namely the assessment of the quality of the classification. There are many evaluation metrics (weâ€™ll learn about them later in the conversation about imbalanced data), but in todayâ€™s example, we only need the simplest of them, the accuracy of the classification. It tells us what percentage of instances in the <em>test set</em> has been correctly classified by us and works well for balanced problems, i.e. those in which the number of instances belonging to both classes is relatively similar.</p>

<p>The classifiers in the <code class="language-plaintext highlighter-rouge">scikit-learn</code> library implement the <code class="language-plaintext highlighter-rouge">score ()</code> method, which allows us to determine the quality of the classification. However, we will use a separate function <code class="language-plaintext highlighter-rouge">accuracy_score()</code>, thanks to which we will obtain this value based on the prediction determined by us in the previous step. In this way, we will get used to making predictions and determining the evaluation metric on our own, which will be necessary in the case of building our own classification models (especially classifier ensembles) and dealing with imbalanced data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score:</span><span class="se">\n</span><span class="s"> %.2f"</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; Accuracy score:
&gt;&gt; 0.93
</code></pre></div></div>

<p>As we can see, our model achieved 93% of the classification accuracy. This is a very good result, but unfortunately it is mainly due to the simplicity of the problem we dealt with today.</p>

<p>Additionally, if we want to learn more about how our model has behaved, we can use the function <code class="language-plaintext highlighter-rouge">confusion_matrix()</code> to find the <em>confusion matrix</em>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Confusion matrix: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; Confusion matrix:
&gt;&gt; [[13  0]
&gt;&gt; [ 2 15]]
</code></pre></div></div>

<p>Thanks to this, we learned that our classifier made two mistakes in classifying an object of a positive class as an object of a negative class (False Negative value = 2).</p>

<p>Phew, thatâ€™s all for today. Soon we will learn how to conduct proper experiments, that is, using <em>cross-validation</em>. Until then, we can be proud of the basic knowledge we have gained today!</p>

<p><img src="/examples/kod2neo.jpg" alt="" /></p>
:ET
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-10-25T21:20:24+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Methods of Computing Intelligence and Decision Making</title><entry><title type="html">Examples 1 — Problems</title><link href="http://localhost:4000/2021/10/25/lab1-eng.html" rel="alternate" type="text/html" title="Examples 1 — Problems" /><published>2021-10-25T00:00:00+02:00</published><updated>2021-10-25T00:00:00+02:00</updated><id>http://localhost:4000/2021/10/25/lab1-eng</id><content type="html" xml:base="http://localhost:4000/2021/10/25/lab1-eng.html">&lt;p&gt;We will start the laboratory as simply as possible. We will generate several synthetic datasets and save them later to CSV files.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;synthetic-data-sets&quot;&gt;Synthetic data sets&lt;/h2&gt;

&lt;p&gt;As a rule of thumb, in real scientific research (serious research that is supposed to be read by someone) we should rather not use synthetic sets – ie. samples generated by an algorithm that do not present any real problem.&lt;/p&gt;

&lt;p&gt;Nevertheless, they turn out to be very useful. They allow us to easily and quickly obtain many different and repeatable (let’s love &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random_state&lt;/code&gt;) problem variants that meet our requirements. For example, we can generate ten different datasets at once, each of which will have exactly thirty objects (a small number of patterns) with a thousand attributes (high dimensionality of the problem).&lt;/p&gt;

&lt;p&gt;A large part of the research work, especially at the beginning of each research, is building prototypes - the first approaches to problems that we should be able to test quickly in order to catch programming and conceptual errors of the method we’ve just invented. Thanks to this, it will be possible to move much faster from the prototyping stage to the construction of a final algorithm, which, of course, will be tested at the end on real datasets.&lt;/p&gt;

&lt;h3 id=&quot;generating-synthetic-sets&quot;&gt;Generating synthetic sets&lt;/h3&gt;

&lt;p&gt;Suffice it to convince you that it’s useful and let’s move on to an example.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the first line of the example, from the &lt;a href=&quot;https://scikit-learn.org/stable/&quot;&gt;scikit-learn library&lt;/a&gt; we import the module &lt;a href=&quot;https://scikit-learn.org/stable/datasets/index.html&quot;&gt;datasets&lt;/a&gt; dedicated for loading a few standard datasets (some are 100 years old, so we won’t bother with them for a while) and for generating synthetic datasets. This is done with the function &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_classification()&lt;/code&gt;&lt;/a&gt;, which we called at the end of this short example. It returns a tuple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(X, y)&lt;/code&gt;, the first element of which is - according to the definitions we already know - &lt;em&gt;dataset&lt;/em&gt;, and the second is &lt;em&gt;a set of labels&lt;/em&gt;. Let’s see what size structures are generated by it by default&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; (100, 20) (100,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are one hundred rows of twenty elements in the data set (matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;). This means that we generated a hundred patterns with twenty attributes each. The label set includes (shocking) labels for each of the patterns.&lt;/p&gt;

&lt;h3 id=&quot;arguments-for-the-make_classification-function&quot;&gt;Arguments for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_classification()&lt;/code&gt; function&lt;/h3&gt;

&lt;p&gt;Of course, we can control each of these parameters. For example, let’s generate a dataset containing one thousand eight-dimensional patterns.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; (1000, 8) (1000,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The next parameters (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_samples&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; n_features&lt;/code&gt;) are passed as arguments to the function. The method implemented in it (originally used to generate &lt;a href=&quot;http://clopinet.com/isabelle/Projects/NIPS2003/Slides/NIPS2003-Datasets.pdf&quot;&gt;Madelon dataset&lt;/a&gt; and developed in 1998 by Ms &lt;a href=&quot;http://clopinet.com/isabelle/&quot;&gt;Isabelle Guyon&lt;/a&gt;) accepts many arguments, but we will only list the ones we find most useful for laboratory and project experiments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_samples&lt;/code&gt; — number of generated patterns (default 100).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_features&lt;/code&gt; — number of problem attributes (default 20).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_classes&lt;/code&gt; — number of problem classes (default 2, so dichotomy).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_clusters_per_class&lt;/code&gt; — the number of centroids for each class, and hence the number of clusters of each problem class (default 2).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flip_y&lt;/code&gt; — the noise level, i.e. the number of labels intentionally misassigned (0.01 by default).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random_state&lt;/code&gt; — integer that allows you to generate exactly the same set in each script repetition (default is None, i.e. a random seed of randomness).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to these basic properties of the data set, it is worth learning how to control the types of generated attributes for starters. In the case of this generator, all features will be quantitative, but we can modify the characteristics of their usefulness in building the model. The relevant arguments in this case are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_informative&lt;/code&gt; - the number of informative attributes, i.e. those that actually contain information useful for classification, not just noise (2 by default).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_redundant&lt;/code&gt; - number of redundant attributes that are combinations of informative features (2 by default).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_repeated&lt;/code&gt; - the number of repeated attributes, i.e. duplicate columns, randomly selected from informative and redundant (2 by default).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By default, therefore, a unique (no random seed) binary problem is generated, which consists of one hundred patterns, and only six of its twenty attributes contain potentially useful information (two informative, two are informative combinations and two repeat random from the previous four), and the rest it consists entirely of noise.&lt;/p&gt;

&lt;h3 id=&quot;lets-invent-a-problem&quot;&gt;Let’s invent a problem&lt;/h3&gt;

&lt;p&gt;Armed with the knowledge of the parameters of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_classification()&lt;/code&gt;, we can finally invent of a problem. Let it be a reproducible (grain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1410&lt;/code&gt;) two-dimensional dataset in which only one attribute is informative (the other has noise) and consists of one hundred patterns with a noise of five percent for the binary problem.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_informative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_repeated&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_redundant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;flip_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1410&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_clusters_per_class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We generated a two-dimensional set in order to be able to draw its &lt;em&gt;scatterplot&lt;/em&gt;. Next, we import the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt; library, prepare a blank 5 by 2.5 inch illustration, draw &lt;em&gt;scatterplot&lt;/em&gt; (giving separate columns with dimensions and specifying the color - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c&lt;/code&gt; - by the generated labels), describe the axes and save the PNG file.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;bwr&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$x^1$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$x^2$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;scatter.png&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/examples/sroda1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see in the attached picture, the first attribute allows to distinguish between classes, so it is informative and the second one contains only noise. Additionally, in both clusters, you can see single objects with an originally incorrect label, making up exactly five objects in total, i.e. five percent of the generated hundred objects.&lt;/p&gt;

&lt;h3 id=&quot;lets-store-the-problem&quot;&gt;Let’s store the problem&lt;/h3&gt;

&lt;p&gt;Finally, we will repeat something that I showed you at the beginning of the second lecture. Data sets are often made available as CSV files. In case someone does not know this format, these are &lt;em&gt;very simple&lt;/em&gt; text files representing arrays, in which each line describes a single record (in our case, a pattern), and each attribute is separated by a comma.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An additional requirement that is often forgotten is to ensure in such a file that each line has exactly the same number of elements (and therefore the same number of separators). A variant of this format is also TSV (tab-separated values), where values are separated by a tab character. If we are stubborn, we can separate the values with any separator we choose, but let’s not charge, for example, to use an asterisk or an exclamation point. No one will understand it, nor will it be of any use to anyone.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, we should join the data set and the label set together, adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; as an additional, last column of the&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; X&lt;/code&gt; matrix. It is purely arbitrary to consider a set of labels as the last column of a set in serialized form, but we can assume that this is convenient solution.&lt;/p&gt;

&lt;p&gt;Here we need &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt; and its built-in function &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;concatenate()&lt;/code&gt;&lt;/a&gt;. As we can read in the documentation, the first attribute (unnamed and positional) takes “&lt;em&gt;a sequence or an array of arrays of the same shape&lt;/em&gt;”. We understand the sequence as the tuple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(X, y)&lt;/code&gt;, although we could safely pass an array there as well.&lt;/p&gt;

&lt;p&gt;Passing it in the simplest form, however, will result in an error resulting from the fact that we did not provide our numpy arrays with a common shape. We should understand it as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the same number of dimensions (vector length returned by the array’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shape&lt;/code&gt; attribute),&lt;/li&gt;
  &lt;li&gt;the same size in all dimensions except one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our last generated dataset has dimensions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(100, 2)&lt;/code&gt; and our label set is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(100)&lt;/code&gt;, so we do not meet the first of these two conditions. We can make up for this by addressing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; in a way that will generate an extra dimension at its end and start treating it not as a vector, but as a matrix with a single column. We can achieve this by calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y[:, e.g.newaxis]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second essential attribute of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;concatenate()&lt;/code&gt; function is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;axis&lt;/code&gt;, which is the axis on which we plan to join the passed arrays together. We should indicate with it in which dimension (which axis) arrays have different sizes. In our case it is the second parameter, so counting from zero we should pass the value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt; there. We can therefore write the correct join as below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can easily save a single matrix in a CSV text file, again using the built-in method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt; [&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;savetxt()&lt;/code&gt;] (https://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html).&lt;/p&gt;

&lt;p&gt;It has two positional arguments. In the first, we give a string with the filename, in the second, the array we want to save. Additionally, if we want to meet CSV format requirements, we should also change the default separator to comma.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savetxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is supposed to be enough, but if we look at the contents of the file, we will see that it does not look very beautiful.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-1.060890925868820389e+00,2.455805509796364028e+00,0.000000000000000000e+00
-6.853148372231210317e-01,-1.500489680108889390e+00,0.000000000000000000e+00
7.775378070067257008e-01,1.330505069088626868e+00,1.000000000000000000e+00
6.933842893245101280e-01,7.217146432694194758e-01,1.000000000000000000e+00
-2.715820952605125793e-01,7.934501868603156538e-01,1.000000000000000000e+00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If one is a lover of scientific notation, of course, he can leave it as it is, but it must be honestly admitted that writing a label, which is ultimately an integer from a cavernous, discrete set of only zero and one, as a number in mathematical notation with precision of eighteen a decimal place is somewhat backbreaking. We can fix it by specifying the number format manually. I won’t explain it in detail right now, but believe it, it works.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savetxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;dataset.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%.5f&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%i&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-1.06089,2.45581,0
-0.68531,-1.50049,0
0.77754,1.33051,1
0.69338,0.72171,1
-0.27158,0.79345,1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Better right now, right?&lt;/p&gt;

&lt;h3 id=&quot;lets-go-back-to-the-old-problem&quot;&gt;Let’s go back to the old problem&lt;/h3&gt;

&lt;p&gt;Okay, finally let’s see how to read CSV files into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt; arrays. Again, we have a built-in function for this, called &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;genfromtxt ()&lt;/code&gt;&lt;/a&gt;. So let’s read the file into the variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dataset&lt;/code&gt; (remembering about the separator) and see how it loaded beautifully.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;genfromtxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; [[-1.06089  2.45581  0.     ]
&amp;gt;&amp;gt;  [-0.68531 -1.50049  0.     ]
&amp;gt;&amp;gt;  [ 0.77754  1.33051  1.     ]
&amp;gt;&amp;gt;  [ 0.69338  0.72171  1.     ]
&amp;gt;&amp;gt;  [-0.27158  0.79345  1.     ]
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since it’s already loaded, let’s divide it into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; y&lt;/code&gt;, assigning all columns except the last one to the first one, and only the last one to the second one. Since the labels are integers anyway, let’s cast it to an integer by the way.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Beautiful. We can already carry out such advanced tasks as coming up with a problem and saving it for later, so that we can always come back to it. Perfectly.&lt;/p&gt;</content><author><name></name></author><summary type="html">We will start the laboratory as simply as possible. We will generate several synthetic datasets and save them later to CSV files.</summary></entry><entry><title type="html">Examples 2 — Fitting classification models</title><link href="http://localhost:4000/2021/10/25/lab2-eng.html" rel="alternate" type="text/html" title="Examples 2 — Fitting classification models" /><published>2021-10-25T00:00:00+02:00</published><updated>2021-10-25T00:00:00+02:00</updated><id>http://localhost:4000/2021/10/25/lab2-eng</id><content type="html" xml:base="http://localhost:4000/2021/10/25/lab2-eng.html">&lt;p&gt;Today we will build a classification model, use it to make a prediction and evaluate how well our classifier copes with a given problem. So let’s not waste time and get to work!&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;dataset-preparation&quot;&gt;Dataset preparation&lt;/h2&gt;

&lt;h3 id=&quot;loading-dataset&quot;&gt;Loading dataset&lt;/h3&gt;

&lt;p&gt;In the previous examples, we learned how to generate a synthetic data set, save it to a CSV file and load such files. So let’s start today’s task by loading the previously prepared dataset, which is located in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dataset.csv&lt;/code&gt; file (conveniently available for download &lt;a href=&quot;https://github.com/metsi/metsi.github.io/blob/master/examples/dataset.csv&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The data set was generated using a (well-known I hope) code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_classification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_informative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_repeated&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_redundant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;flip_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1410&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_clusters_per_class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Loading the problem and its division into &lt;em&gt;data set&lt;/em&gt; and &lt;em&gt;set of labels&lt;/em&gt;, as a reminder, is performed as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;genfromtxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have our ‘X’ and ‘y’ ready, but if we want our research to bear the hallmarks of reliability, then we cannot stop there.&lt;/p&gt;

&lt;h3 id=&quot;division-of-data-into-a-training-and-a-test-set&quot;&gt;Division of data into a training and a test set&lt;/h3&gt;

&lt;p&gt;Before doing any research, we must remember to divide our dataset into &lt;em&gt;training set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt;. In the classification task, the &lt;em&gt;training set&lt;/em&gt; contains the instances of the problem known to us, which have been previously labeled and will be used to train the model we have chosen, so that it can then generalize the acquired knowledge in relation to instances that it has not seen before (i.e. &lt;em&gt;test set&lt;/em&gt;). Training and testing a model on the same data set is absolutely unacceptable and the results obtained in this way do not in any way reflect the actual predictive ability of the classifier.&lt;/p&gt;

&lt;p&gt;In today’s example, we will use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_test_split()&lt;/code&gt; function, with which we will perform a single partition of our data set. This approach is clearly intellectually unobtrusive and insulting to the dignity of a scientist, but we will present it here (&lt;strong&gt;and only here&lt;/strong&gt;) as the simplest possible example of data division into the classification task. Next time, we will learn to use &lt;em&gt;cross-validation&lt;/em&gt;, which will allow us to conduct our research reliably, and this is what we will use in all future machine learning encounters.&lt;/p&gt;

&lt;p&gt;But back to the present, let’s prepare our &lt;em&gt;test set&lt;/em&gt; and &lt;em&gt;training set&lt;/em&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_test_split()&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, we only need to pass two parameters as arguments to the function, i.e. well-known and liked &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random_state&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test_size&lt;/code&gt;, which tells us what percentage of all data will be used to test our model. Here we decide to use 70% of the data for training and 30% for testing the classifier.&lt;/p&gt;

&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;

&lt;h3 id=&quot;initialization-and-construction-of-classification-model&quot;&gt;Initialization and construction of classification model&lt;/h3&gt;

&lt;p&gt;Now that we have our data ready, we can finally move on to initializing and training the classification model. We will use a simple probabilistic classifier, which is the &lt;em&gt;Naive Bayes classifier&lt;/em&gt; in the version based on the normal distribution (Gaussian distribution). It is called &lt;em&gt;naive&lt;/em&gt; because it assumes that the problem features are independent of each other and is implemented by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GaussianNB&lt;/code&gt; class.&lt;/p&gt;

&lt;p&gt;In order to fit our model to the training data, we call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit()&lt;/code&gt; method, which takes the &lt;em&gt;data set&lt;/em&gt; and &lt;em&gt;label set&lt;/em&gt; of our training set:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GaussianNB&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GaussianNB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our model has already been learned from the labeled data, so we can finally proceed with the prediction.&lt;/p&gt;

&lt;h3 id=&quot;determination-of-the-support-and-prediction-matrix&quot;&gt;Determination of the support and prediction matrix&lt;/h3&gt;

&lt;p&gt;The classifiers implemented in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; library implement the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict()&lt;/code&gt; method, which allows us to directly obtain labels assigned to unknown instances by the classifier. However, to prevent it from being so easy, we will not use it.&lt;/p&gt;

&lt;p&gt;Due to the fact that the model used by us is probabilistic, we can ask him to return the &lt;em&gt;support matrix&lt;/em&gt;. The number of rows in this matrix is equal to the number of problem instances in the &lt;em&gt;test set&lt;/em&gt; (i.e. we have 30), and the number of columns is equal to the number of problem classes (in our case, the problem is binary, so we have two columns). Each row of the matrix corresponds to &lt;em&gt;the support vector&lt;/em&gt; returned by the model for a given classified problem instance. The values in this vector tell us with what probability the classified instance belongs to a given class. &lt;em&gt;The support matrix&lt;/em&gt; can be obtained using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict_proba()&lt;/code&gt; method and giving our test data set as its argument:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;class_probabilities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Class probabilities:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; Class probabilities:
&amp;gt;&amp;gt; [[2.57883436e-06 9.99997421e 01]
&amp;gt;&amp;gt; [1.99658559e-06 9.99998003e-01]
&amp;gt;&amp;gt; [9.90973563e-01 9.02643748e-03]
&amp;gt;&amp;gt; [5.47593735e-05 9.99945241e-01]
&amp;gt;&amp;gt; [9.98382405e-01 1.61759531e-03]
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Having knowledge of the probabilities of the &lt;em&gt;test set&lt;/em&gt; instance belonging to classes (posterior probability), we can easily calculate the prediction. We will do this assuming that each of the test instances belongs to the problem class for which the probability of belonging is the highest.&lt;/p&gt;

&lt;p&gt;We can do this very efficiently with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argmax()&lt;/code&gt; function, which will return to us the index of the column with the largest value for each row. For this purpose, we give as arguments the previously obtained &lt;em&gt;support matrix&lt;/em&gt; and the axis on which we want to move:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_probabilities&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Predicted labels:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; Predicted labels:
&amp;gt;&amp;gt; [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For purely illustrative purposes, we will now list the true labels of our &lt;em&gt;test set&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_test&lt;/code&gt;) and our prediction (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt;) next to each other:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;True labels:     &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Predicted labels:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; True labels:      [1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
&amp;gt;&amp;gt; Predicted labels: [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we are dealing here with only 30 instances of the problem, at first glance we can say that the two vectors are slightly different from each other. This shows that our prediction is not perfect (and in practice it will never be perfect), but of course it would be inconvenient to judge how our model works in this way. We need a specific value that will tell us how good our classifier is in this case.&lt;/p&gt;

&lt;h3 id=&quot;assessment-of-the-classification-quality-of-our-model&quot;&gt;Assessment of the classification quality of our model&lt;/h3&gt;

&lt;p&gt;We managed to get to the last issue of today, namely the assessment of the quality of the classification. There are many evaluation metrics (we’ll learn about them later in the conversation about imbalanced data), but in today’s example, we only need the simplest of them, the accuracy of the classification. It tells us what percentage of instances in the &lt;em&gt;test set&lt;/em&gt; has been correctly classified by us and works well for balanced problems, i.e. those in which the number of instances belonging to both classes is relatively similar.&lt;/p&gt;

&lt;p&gt;The classifiers in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt; library implement the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;score ()&lt;/code&gt; method, which allows us to determine the quality of the classification. However, we will use a separate function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accuracy_score()&lt;/code&gt;, thanks to which we will obtain this value based on the prediction determined by us in the previous step. In this way, we will get used to making predictions and determining the evaluation metric on our own, which will be necessary in the case of building our own classification models (especially classifier ensembles) and dealing with imbalanced data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy score:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; %.2f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; Accuracy score:
&amp;gt;&amp;gt; 0.93
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As we can see, our model achieved 93% of the classification accuracy. This is a very good result, but unfortunately it is mainly due to the simplicity of the problem we dealt with today.&lt;/p&gt;

&lt;p&gt;Additionally, if we want to learn more about how our model has behaved, we can use the function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;confusion_matrix()&lt;/code&gt; to find the &lt;em&gt;confusion matrix&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Confusion matrix: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt; Confusion matrix:
&amp;gt;&amp;gt; [[13  0]
&amp;gt;&amp;gt; [ 2 15]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thanks to this, we learned that our classifier made two mistakes in classifying an object of a positive class as an object of a negative class (False Negative value = 2).&lt;/p&gt;

&lt;p&gt;Phew, that’s all for today. Soon we will learn how to conduct proper experiments, that is, using &lt;em&gt;cross-validation&lt;/em&gt;. Until then, we can be proud of the basic knowledge we have gained today!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/examples/kod2neo.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>tibetansandfox</name></author><summary type="html">Today we will build a classification model, use it to make a prediction and evaluate how well our classifier copes with a given problem. So let’s not waste time and get to work!</summary></entry></feed>
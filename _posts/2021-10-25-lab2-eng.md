---
layout: post
title: Examples 2 â€” Fitting classification models
excerpt_separator: <!--more-->
color: rgb(165,42,42)
author: tibetansandfox
date: 2021-10-25
---

Today we will build a classification model, use it to make a prediction and evaluate how well our classifier copes with a given problem. So let's not waste time and get to work!

<!--more-->

## Dataset preparation

### Loading dataset

In the previous examples, we learned how to generate a synthetic data set, save it to a CSV file and load such files. So let's start today's task by loading the previously prepared dataset, which is located in the `dataset.csv` file (conveniently available for download [here](https://github.com/metsi/metsi.github.io/blob/master/examples/dataset.csv)).

The data set was generated using a (well-known I hope) code:

```python
from sklearn.datasets import make_classification
X, y = datasets.make_classification(
    n_samples=100,
    n_features=2,
    n_informative=1,
    n_repeated=0,
    n_redundant=0,
    flip_y=.05,
    random_state=1410,
    n_clusters_per_class=1
)
```

Loading the problem and its division into *data set* and *set of labels*, as a reminder, is performed as follows:

```python
import numpy as np
dataset = np.genfromtxt("dataset.csv", delimiter=",")
X = dataset[:, :-1]
y = dataset[:, -1].astype(int)
```

We have our 'X' and 'y' ready, but if we want our research to bear the hallmarks of reliability, then we cannot stop there.

### Division of data into a training and a test set

Before doing any research, we must remember to divide our dataset into *training set* and *test set*. In the classification task, the *training set* contains the instances of the problem known to us, which have been previously labeled and will be used to train the model we have chosen, so that it can then generalize the acquired knowledge in relation to instances that it has not seen before (i.e. *test set*). Training and testing a model on the same data set is absolutely unacceptable and the results obtained in this way do not in any way reflect the actual predictive ability of the classifier.

In today's example, we will use the `train_test_split()` function, with which we will perform a single partition of our data set. This approach is clearly intellectually unobtrusive and insulting to the dignity of a scientist, but we will present it here (**and only here**) as the simplest possible example of data division into the classification task. Next time, we will learn to use *cross-validation*, which will allow us to conduct our research reliably, and this is what we will use in all future machine learning encounters.

But back to the present, let's prepare our *test set* and *training set* using `train_test_split()`:

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=.3,
    random_state=42
)
```

In this case, we only need to pass two parameters as arguments to the function, i.e. well-known and liked `random_state` and `test_size`, which tells us what percentage of all data will be used to test our model. Here we decide to use 70% of the data for training and 30% for testing the classifier.

## Classification

### Initialization and construction of classification model

Now that we have our data ready, we can finally move on to initializing and training the classification model. We will use a simple probabilistic classifier, which is the *Naive Bayes classifier* in the version based on the normal distribution (Gaussian distribution). It is called *naive* because it assumes that the problem features are independent of each other and is implemented by the `GaussianNB` class.

In order to fit our model to the training data, we call the `fit()` method, which takes the *data set* and *label set* of our training set:

```python
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)
```

Our model has already been learned from the labeled data, so we can finally proceed with the prediction.

### Determination of the support and prediction matrix

The classifiers implemented in the `scikit-learn` library implement the `predict()` method, which allows us to directly obtain labels assigned to unknown instances by the classifier. However, to prevent it from being so easy, we will not use it.

Due to the fact that the model used by us is probabilistic, we can ask him to return the *support matrix*. The number of rows in this matrix is equal to the number of problem instances in the *test set* (i.e. we have 30), and the number of columns is equal to the number of problem classes (in our case, the problem is binary, so we have two columns). Each row of the matrix corresponds to *the support vector* returned by the model for a given classified problem instance. The values in this vector tell us with what probability the classified instance belongs to a given class. *The support matrix* can be obtained using the `predict_proba()` method and giving our test data set as its argument:

```python
class_probabilities = clf.predict_proba(X_test)
print("Class probabilities:\n", class_probabilities)
```
```
>> Class probabilities:
>> [[2.57883436e-06 9.99997421e 01]
>> [1.99658559e-06 9.99998003e-01]
>> [9.90973563e-01 9.02643748e-03]
>> [5.47593735e-05 9.99945241e-01]
>> [9.98382405e-01 1.61759531e-03]
...
```

Having knowledge of the probabilities of the *test set* instance belonging to classes (posterior probability), we can easily calculate the prediction. We will do this assuming that each of the test instances belongs to the problem class for which the probability of belonging is the highest.

We can do this very efficiently with the `argmax()` function, which will return to us the index of the column with the largest value for each row. For this purpose, we give as arguments the previously obtained *support matrix* and the axis on which we want to move:

```python
predict = np.argmax(class_probabilities, axis=1)
print("Predicted labels:\n", predict)
```
```
>> Predicted labels:
>> [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
```

For purely illustrative purposes, we will now list the true labels of our *test set* (`y_test`) and our prediction (`predict`) next to each other:

```python
print("True labels:     ", y_test)
print("Predicted labels:", predict)
```
```
>> True labels:      [1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
>> Predicted labels: [1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0]
```

As we are dealing here with only 30 instances of the problem, at first glance we can say that the two vectors are slightly different from each other. This shows that our prediction is not perfect (and in practice it will never be perfect), but of course it would be inconvenient to judge how our model works in this way. We need a specific value that will tell us how good our classifier is in this case.

### Assessment of the classification quality of our model

We managed to get to the last issue of today, namely the assessment of the quality of the classification. There are many evaluation metrics (we'll learn about them later in the conversation about imbalanced data), but in today's example, we only need the simplest of them, the accuracy of the classification. It tells us what percentage of instances in the *test set* has been correctly classified by us and works well for balanced problems, i.e. those in which the number of instances belonging to both classes is relatively similar.

The classifiers in the `scikit-learn` library implement the `score ()` method, which allows us to determine the quality of the classification. However, we will use a separate function `accuracy_score()`, thanks to which we will obtain this value based on the prediction determined by us in the previous step. In this way, we will get used to making predictions and determining the evaluation metric on our own, which will be necessary in the case of building our own classification models (especially classifier ensembles) and dealing with imbalanced data.

```python
from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, predict)
print("Accuracy score:\n %.2f" % score)
```
```
>> Accuracy score:
>> 0.93
```

As we can see, our model achieved 93% of the classification accuracy. This is a very good result, but unfortunately it is mainly due to the simplicity of the problem we dealt with today.

Additionally, if we want to learn more about how our model has behaved, we can use the function `confusion_matrix()` to find the *confusion matrix*:

```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, predict)
print("Confusion matrix: \n", cm)
```
```
>> Confusion matrix:
>> [[13  0]
>> [ 2 15]]
```

Thanks to this, we learned that our classifier made two mistakes in classifying an object of a positive class as an object of a negative class (False Negative value = 2).

Phew, that's all for today. Soon we will learn how to conduct proper experiments, that is, using *cross-validation*. Until then, we can be proud of the basic knowledge we have gained today!

![](/examples/kod2neo.jpg)

---
layout: post
title: Examples 1 — Problems
excerpt_separator: <!--more-->
date: 2021-10-25
---

We will start the laboratory as simply as possible. We will generate several synthetic datasets and save them later to CSV files.

<!--more-->

## Synthetic data sets

As a rule of thumb, in real scientific research (serious research that is supposed to be read by someone) we should rather not use synthetic sets -- ie. samples generated by an algorithm that do not present any real problem.

Nevertheless, they turn out to be very useful. They allow us to easily and quickly obtain many different and repeatable (let's love `random_state`) problem variants that meet our requirements. For example, we can generate ten different datasets at once, each of which will have exactly thirty objects (a small number of patterns) with a thousand attributes (high dimensionality of the problem).

A large part of the research work, especially at the beginning of each research, is building prototypes - the first approaches to problems that we should be able to test quickly in order to catch programming and conceptual errors of the method we've just invented. Thanks to this, it will be possible to move much faster from the prototyping stage to the construction of a final algorithm, which, of course, will be tested at the end on real datasets.

### Generating synthetic sets

Suffice it to convince you that it's useful and let's move on to an example.

```python
from sklearn import datasets
X, y = datasets.make_classification()
```

In the first line of the example, from the [scikit-learn library](https://scikit-learn.org/stable/) we import the module [datasets](https://scikit-learn.org/stable/datasets/index.html) dedicated for loading a few standard datasets (some are 100 years old, so we won't bother with them for a while) and for generating synthetic datasets. This is done with the function [`make_classification()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html), which we called at the end of this short example. It returns a tuple `(X, y)`, the first element of which is - according to the definitions we already know - *dataset*, and the second is *a set of labels*. Let's see what size structures are generated by it by default

```python
print(X.shape, y.shape)
```
```
>> (100, 20) (100,)
```

There are one hundred rows of twenty elements in the data set (matrix `X`). This means that we generated a hundred patterns with twenty attributes each. The label set includes (shocking) labels for each of the patterns.

### Arguments for the `make_classification()` function

Of course, we can control each of these parameters. For example, let's generate a dataset containing one thousand eight-dimensional patterns.

```python
X, y = datasets.make_classification(n_samples=1000, n_features=8)
print(X.shape, y.shape)
```
```
>> (1000, 8) (1000,)
```

The next parameters (`n_samples`,` n_features`) are passed as arguments to the function. The method implemented in it (originally used to generate [Madelon dataset](http://clopinet.com/isabelle/Projects/NIPS2003/Slides/NIPS2003-Datasets.pdf) and developed in 1998 by Ms [Isabelle Guyon](http://clopinet.com/isabelle/)) accepts many arguments, but we will only list the ones we find most useful for laboratory and project experiments:

- `n_samples` — number of generated patterns (default 100).
- `n_features` — number of problem attributes (default 20).
- `n_classes` — number of problem classes (default 2, so dichotomy).
- `n_clusters_per_class` — the number of centroids for each class, and hence the number of clusters of each problem class (default 2).
- `flip_y` — the noise level, i.e. the number of labels intentionally misassigned (0.01 by default).
- `random_state` — integer that allows you to generate exactly the same set in each script repetition (default is None, i.e. a random seed of randomness).

In addition to these basic properties of the data set, it is worth learning how to control the types of generated attributes for starters. In the case of this generator, all features will be quantitative, but we can modify the characteristics of their usefulness in building the model. The relevant arguments in this case are:

- `n_informative` - the number of informative attributes, i.e. those that actually contain information useful for classification, not just noise (2 by default).
- `n_redundant` - number of redundant attributes that are combinations of informative features (2 by default).
- `n_repeated` - the number of repeated attributes, i.e. duplicate columns, randomly selected from informative and redundant (2 by default).

By default, therefore, a unique (no random seed) binary problem is generated, which consists of one hundred patterns, and only six of its twenty attributes contain potentially useful information (two informative, two are informative combinations and two repeat random from the previous four), and the rest it consists entirely of noise.

### Let's invent a problem

Armed with the knowledge of the parameters of `make_classification()`, we can finally invent of a problem. Let it be a reproducible (grain `1410`) two-dimensional dataset in which only one attribute is informative (the other has noise) and consists of one hundred patterns with a noise of five percent for the binary problem.

```python
X, y = datasets.make_classification(
    n_samples=100,
    n_features=2,
    n_informative=1,
    n_repeated=0,
    n_redundant=0,
    flip_y=.05,
    random_state=1410,
    n_clusters_per_class=1
)
```

We generated a two-dimensional set in order to be able to draw its *scatterplot*. Next, we import the `matplotlib` library, prepare a blank 5 by 2.5 inch illustration, draw *scatterplot* (giving separate columns with dimensions and specifying the color - `c` - by the generated labels), describe the axes and save the PNG file.

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(5,2.5))
plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr')
plt.xlabel("$x^1$")
plt.ylabel("$x^2$")
plt.tight_layout()
plt.savefig('scatter.png')
```

![](/examples/sroda1.png)

As you can see in the attached picture, the first attribute allows to distinguish between classes, so it is informative and the second one contains only noise. Additionally, in both clusters, you can see single objects with an originally incorrect label, making up exactly five objects in total, i.e. five percent of the generated hundred objects.

### Let's store the problem

Finally, we will repeat something that I showed you at the beginning of the second lecture. Data sets are often made available as CSV files. In case someone does not know this format, these are *very simple* text files representing arrays, in which each line describes a single record (in our case, a pattern), and each attribute is separated by a comma.

> An additional requirement that is often forgotten is to ensure in such a file that each line has exactly the same number of elements (and therefore the same number of separators). A variant of this format is also TSV (tab-separated values), where values are separated by a tab character. If we are stubborn, we can separate the values with any separator we choose, but let's not charge, for example, to use an asterisk or an exclamation point. No one will understand it, nor will it be of any use to anyone.

First, we should join the data set and the label set together, adding `y` as an additional, last column of the` X` matrix. It is purely arbitrary to consider a set of labels as the last column of a set in serialized form, but we can assume that this is convenient solution.

Here we need `numpy` and its built-in function [`concatenate()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html). As we can read in the documentation, the first attribute (unnamed and positional) takes "*a sequence or an array of arrays of the same shape*". We understand the sequence as the tuple `(X, y)`, although we could safely pass an array there as well.

Passing it in the simplest form, however, will result in an error resulting from the fact that we did not provide our numpy arrays with a common shape. We should understand it as:

- the same number of dimensions (vector length returned by the array's `shape` attribute),
- the same size in all dimensions except one.

Our last generated dataset has dimensions of `(100, 2)` and our label set is `(100)`, so we do not meet the first of these two conditions. We can make up for this by addressing `y` in a way that will generate an extra dimension at its end and start treating it not as a vector, but as a matrix with a single column. We can achieve this by calling `y[:, e.g.newaxis]`.

The second essential attribute of the `concatenate()` function is `axis`, which is the axis on which we plan to join the passed arrays together. We should indicate with it in which dimension (which axis) arrays have different sizes. In our case it is the second parameter, so counting from zero we should pass the value `1` there. We can therefore write the correct join as below:

```python
import numpy as np
dataset = np.concatenate((X, y[:, np.newaxis]), axis=1)
```

We can easily save a single matrix in a CSV text file, again using the built-in method `numpy` [`savetxt()`] (https://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html).

It has two positional arguments. In the first, we give a string with the filename, in the second, the array we want to save. Additionally, if we want to meet CSV format requirements, we should also change the default separator to comma.

```python
np.savetxt("dataset.csv", dataset, delimiter=",")
```

This is supposed to be enough, but if we look at the contents of the file, we will see that it does not look very beautiful.

```
-1.060890925868820389e+00,2.455805509796364028e+00,0.000000000000000000e+00
-6.853148372231210317e-01,-1.500489680108889390e+00,0.000000000000000000e+00
7.775378070067257008e-01,1.330505069088626868e+00,1.000000000000000000e+00
6.933842893245101280e-01,7.217146432694194758e-01,1.000000000000000000e+00
-2.715820952605125793e-01,7.934501868603156538e-01,1.000000000000000000e+00
```

If one is a lover of scientific notation, of course, he can leave it as it is, but it must be honestly admitted that writing a label, which is ultimately an integer from a cavernous, discrete set of only zero and one, as a number in mathematical notation with precision of eighteen a decimal place is somewhat backbreaking. We can fix it by specifying the number format manually. I won't explain it in detail right now, but believe it, it works.

```python
np.savetxt(
    "dataset.csv",
    dataset,
    delimiter=",",
    fmt=["%.5f" for i in range(X.shape[1])] + ["%i"],
)
```

```
-1.06089,2.45581,0
-0.68531,-1.50049,0
0.77754,1.33051,1
0.69338,0.72171,1
-0.27158,0.79345,1
```

Better right now, right?

### Let's go back to the old problem

Okay, finally let's see how to read CSV files into `numpy` arrays. Again, we have a built-in function for this, called [`genfromtxt ()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html). So let's read the file into the variable `dataset` (remembering about the separator) and see how it loaded beautifully.

```python
dataset = np.genfromtxt("dataset.csv", delimiter=",")
print(dataset)
```
```
>> [[-1.06089  2.45581  0.     ]
>>  [-0.68531 -1.50049  0.     ]
>>  [ 0.77754  1.33051  1.     ]
>>  [ 0.69338  0.72171  1.     ]
>>  [-0.27158  0.79345  1.     ]
...
```

Since it's already loaded, let's divide it into `X` and` y`, assigning all columns except the last one to the first one, and only the last one to the second one. Since the labels are integers anyway, let's cast it to an integer by the way.

```python
X = dataset[:, :-1]
y = dataset[:, -1].astype(int)
```

Beautiful. We can already carry out such advanced tasks as coming up with a problem and saving it for later, so that we can always come back to it. Perfectly.

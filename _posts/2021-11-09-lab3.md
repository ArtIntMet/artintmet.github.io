---
layout: post
title: Examples 3 â€” Cross-validation
excerpt_separator: <!--more-->
color: rgb(165,42,42)
author: tibetansandfox
date: 2021-11-09
---

## Basic validation

### Data

From the previous examples, we definitely know how to generate or read data. For a quick reminder, below is an excerpt from the data generation code. A minor change is made and we're adding a `weights` parameter, which we'll learn more about later.

```python
from sklearn.datasets import make_classification
X, y = datasets.make_classification(
    weights=[0.8,0.2],
    n_samples=100,
    n_features=2,
    n_informative=1,
    n_repeated=0,
    n_redundant=0,
    flip_y=.05,
    random_state=1234,
    n_clusters_per_class=1
)
```

### Data division

Performing model validation requires us to divide the data into *training set* and *test set*. Training and testing the model on the same data is pointless, and the results obtained in this way will not allow us to determine the quality of our classifier. The simplest example is to use the `train_test_split()` function, which will allow you to perform a single split of our data set. The `test_size` parameter set to `.30` means that 30% of randomly selected samples from the set will be allocated to the *test set*, and 70% to the *training set*. Remember about `random_state`.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=.30,
    random_state=1234
)
```

### Model building and testing

Having a *training set*, we can build a model. The classifier used for this is the *Gaussian Naive Bayes* classifier known from the previous example, based on the normal distribution, also known as the Gaussian distribution. We will do this with the `fit()` method, which takes the `X_train` attributes and the `y_train` labels of the training set as arguments. The next step is to make a prediction on the *test set*. This is done with the `predict()` method, which takes the attributes of the training dataset `X_test` as an argument. The set of predicted labels `predict` is returned, which is the response of our previously learned model. The last step is to calculate the quality based on the selected metric, in this case `accuracy_score`.

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
clf = GaussianNB()
clf.fit(X_train, y_train)
predict = clf.predict(X_test)
score = accuracy_score(y_test, predict)
print("Accuracy score:\n%.2f" % score)
```

The quality we received is quite high, but it does not mean that we are satisfied with this result.

```
Accuracy score: 0.900
```

## Cross-validation

### K-fold cross-validation

We now come to a much better method of validation - k-fold cross-validation. This will allow us to determine the quality of the model in the most frequently used way. The main idea is to divide our set into *K* equal parts. Then, in turn, each of these parts is used as a *test set*, and the rest as a *training set*. This process is performed *K* times and we build a new model from scratch each time. Then the obtained results (e.g. accuracy) are averaged or in some other way aggregated into a single result. Below is a visualization of the split for 5-fold cross-validation. To put it simply, the whole thing is our data set divided into 5 random parts. Each green square means the part selected as *test subset*, and yellow means *training subset*. We have 5 iterations in total.

![5kfold](/examples/5kfold.png)

In practice, it looks like this. First, after importing the appropriate components, we create a `kf` object of the` KFold` class with the number of parts parameter `n_splits`. In this case, we set the value to 5. Remember that the `shuffle` parameter is set to the value `True`, which means that the data will be shuffled before the split, and that the `random_state` parameter is set. Then we create an empty `scores` list, on which we will later put the obtained accuracy on individual *test subsets*.

```python
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=1234)
scores = []
```

In practice, it looks like this. First, after importing the appropriate components, we create a `kf` object of the` KFold` class with the number of parts parameter `n_splits`. In this case, we set the value to 5. Remember that the `shuffle` parameter is set to the value `True`, which means that the data will be shuffled before the split, and that the `random_state` parameter is set. Then we create an empty `scores` list, on which we will later put the obtained accuracy on individual *test subsets*.

```python
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    predict = clf.predict(X_test)
    scores.append(accuracy_score(y_test, predict))
```

In the last step, we average the obtained results (`mean()`) and calculate the standard deviation (`std()`). We do this using the functions contained in the `numpy` library.

```python
import numpy as np
mean_score = np.mean(scores)
std_score = np.std(scores)
print("Accuracy score: %.3f (%.3f)" % (mean_score, std_score))
```

The result obtained by us is:

```
Accuracy score: 0.880 (0.051)
```

![cv](/examples/cv.jpeg)

- Does this mean that we have degraded the quality of our model?

- Firmly no!

We certainly improved the quality of our research. Or, there may be times when the measured accuracy changes, but does not always deteriorate.

### Stratified cross-validation

We can often come across data where the number of objects of a given class is greater than that of other classes. In the case of minor differences, it does not have a large effect on the result and performance of the test. However, the big differences become a bit of a handicap. This is called *data imbalance*. This involves applying an appropriate approach when classifying and conducting experiments. With a simple command we can write out all the labels:

```python
print(y)
```

What causes the discharge:

```
[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0
 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0]
```

At first glance, there are more class "0" labels than "1" labels. We can save ourselves manual counting and use one of the functions of the `numpy` library to find out exactly how many individual values there are.

```python
print(np.unique(y, return_counts=True))
```

The result of which is:

```
(array([0, 1]), array([76, 24]))
```

This means that in our dataset there are exactly 76 samples labeled "0" and 24 labeled "1". Our data is also imbalanced. We call this ratio between the number of class labels *the imbalance level*. The generator parameter `weights = [0.8,0.2]` is responsible for this, by means of which we determine the share of particular classes in the generated data. Cross-validation randomly divides the sub-collection, which may disturb this distribution. At this point, stratified cross-validation that retains the original or close to the original *imbalance* level when partitioning into subsets is very useful.

```python
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)
scores = []

for train_index, test_index in skf.split(X,y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    predict = clf.predict(X_test)
    scores.append(accuracy_score(y_test, predict))

mean_score = np.mean(scores)
std_score = np.std(scores)
print("Accuracy score: %.3f (%.3f)" % (mean_score, std_score))
```

The whole procedure is similar to the usual k-fold cross-validation. We only use a different class in this case, `StratifiedKFold` and in the` split` method we pass the data labels `y`, so that the validator extracts information about the *level of imbalance* of the classes. Finally, we check the accuracy.

```
Accuracy score: 0.860 (0.066)
```

The result is lower, but the research has gained even more quality and we are richer in new knowledge.

### Repeated cross-validation

Yet another approach is multiple cross-validation. The main assumption for this method is that we do a few repetitions of partitioning into subsets. In other words, it is some automation of the cross-validation process several times and calculation of the average value of all these subsets. Practically the code of the program looks almost the same as in the case of normal validation, the difference is only in using a different class `RepeatedKFold` and setting an additional parameter `n_repeats`. In the example below, we will perform a 5-fold cross-validation with 10 repetitions. Simply put, you can say that we are doing 10x5 cross-validation.

```python
from sklearn.model_selection import RepeatedKFold
rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1234)
scores = []

for train_index, test_index in rkf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    predict = clf.predict(X_test)
    scores.append(accuracy_score(y_test, predict))

mean_score = np.mean(scores)
std_score = np.std(scores)
print("Accuracy score: %.3f (%.3f)" % (mean_score, std_score))
```

There is also a repeated stratified cross-validation. Sample program code below.

```python
from sklearn.model_selection import RepeatedStratifiedKFold
rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1234)
scores = []

for train_index, test_index in rskf.split(X,y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    predict = clf.predict(X_test)
    scores.append(accuracy_score(y_test, predict))

mean_score = np.mean(scores)
std_score = np.std(scores)
print("Accuracy score: %.3f (%.3f)" % (mean_score, std_score))
```

### Leave-one-out

The last example of cross-validation today is leave-one-out. This is a very special variation of k-fold cross-validation where a data set is split into subsets containing only one data object each. As in the previous cross-validations, each time one subset (one object here) is used for testing and the rest for training. This type of method is most often used for small data sets. From the practical side, the application is quite analogous to the other examples of today.

```python
from sklearn.model_selection import LeaveOneOut
loo = LeaveOneOut()

for train_index, test_index in loo.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    predict = clf.predict(X_test)
    scores.append(accuracy_score(y_test, predict))
mean_score = np.mean(scores)
std_score = np.std(scores)
print("Accuracy score: %.3f (%.3f)" % (mean_score, std_score))
```
